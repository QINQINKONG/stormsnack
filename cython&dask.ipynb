{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython\n",
    "<img src=\"../images/cython.jpg\" \n",
    "     align=\"right\"\n",
    "     width=\"20%\">\n",
    "## Build Cython code\n",
    "**Two stages:**\n",
    "- use Cython compiler to compile cython source file (```.pyx```) into C code (```.c```)\n",
    "- use C compiler to compile ```.c``` file into ```.so``` file\n",
    "\n",
    "**Several ways:**\n",
    "- setup tools (more flexible, powerful)\n",
    "- jupyter notebook: Cython compilation interactively (more interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using setup tools\n",
    "- write a ```setup.py``` file\n",
    "- ```python setup.py build_ext --inplace```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2-x\n",
    "\n",
    "def integrate_f_py(a, b, N):\n",
    "    s = 0\n",
    "    dx = (b-a)/N\n",
    "    for i in range(N):\n",
    "        s += f(a+i*dx)\n",
    "    return s * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from integrate import integrate_f_cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrate_f_cy(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrate_f_py(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del integrate_f_cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --annotate\n",
    "def f_cy(x):\n",
    "    return x**2-x\n",
    "\n",
    "def integrate_f_cy(a, b, N):\n",
    "    s = 0\n",
    "    dx = (b-a)/N\n",
    "    for i in range(N):\n",
    "        s += f_cy(a+i*dx)\n",
    "    return s * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit integrate_f_py(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit integrate_f_cy(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed up by **30%** without doing anything: removal of interpreter overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static typing:\n",
    "#### typing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "def f_static(double x):\n",
    "    return x**2-x\n",
    "\n",
    "def integrate_f_static(double a, double b, int N):\n",
    "    cdef int i\n",
    "    cdef double s, dx\n",
    "    s = 0\n",
    "    dx = (b-a)/N\n",
    "    for i in range(N):\n",
    "        s += f_static(a+i*dx)\n",
    "    return s * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit integrate_f_static(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 times speed up!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### typing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "cdef double f_static2(double x):\n",
    "    return x**2-x\n",
    "\n",
    "def integrate_f_static2(double a, double b, int N):\n",
    "    cdef int i\n",
    "    cdef double s\n",
    "    s = 0\n",
    "    dx = (b-a)*N**(-1)\n",
    "    for i in range(N):\n",
    "        s += f_static2(a+i*dx)\n",
    "    return s * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit integrate_f_static2(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**200 times speed up!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```cdef``` function can only be called within cython: fast supporting functions\n",
    "- ```def``` functions can be called in python session: function that you want to import in python\n",
    "- ```cpdef``` function can be called both within cython (as C functions) and python (python wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_static2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No need to type everything:\n",
    "- cython enables automatic type inference during assignmnet\n",
    "- Unnecessary typing may even slow things down (unnecessary type checks or conversions)\n",
    "- Must type in performance critical part of the code (such as ```for``` loop: ```for``` loop needs to be white! ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare with Numba ```@njit```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def f_njit(x):\n",
    "    return x**2-x\n",
    "@njit\n",
    "def integrate_f_njit(a,b, N):\n",
    "    s = 0\n",
    "    dx = (b-a)/N\n",
    "    for i in range(N):\n",
    "        s += f_njit(a+i*dx)\n",
    "    return s * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit integrate_f_njit(1, 100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using C libaries\n",
    "\n",
    "- Cython makes calling external C library functions easier! \n",
    "- Cython already defines many standard C libarary functions for us!\n",
    "\n",
    "```python\n",
    "from libc.math cimport sin\n",
    "cdef double f(double x):\n",
    "    return sin(x * x)\n",
    "```\n",
    "\n",
    "https://github.com/cython/cython/blob/master/Cython/Includes/libc/math.pxd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython for Numpy user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.random.randn(100,100)\n",
    "b=np.random.randn(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numpy(array_1,array_2):\n",
    "    return array_1+array_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_numpy(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "def add_cy(array_1,array_2):\n",
    "    return array_1+array_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_cy(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_py(array_1, array_2):\n",
    "    x_max = array_1.shape[0]\n",
    "    y_max = array_1.shape[1]\n",
    "    result = np.zeros((x_max, y_max), dtype=array_1.dtype)\n",
    "    for x in range(x_max):\n",
    "        for y in range(y_max):\n",
    "            result[x, y] = array_1[x, y]+array_2[x, y]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_py(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "def add_cy1(array_1, array_2):\n",
    "    x_max = array_1.shape[0]\n",
    "    y_max = array_1.shape[1]\n",
    "    result = np.zeros((x_max, y_max), dtype=array_1.dtype)\n",
    "    for x in range(x_max):\n",
    "        for y in range(y_max):\n",
    "            result[x, y] = array_1[x, y]+array_2[x, y]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_cy1(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typed memoryview\n",
    "- **Memoryview:** memoryviews are C structures that can hold a pointer to the data of a NumPy array and all the necessary buffer metadata to provide efficient and safe access: dimensions, strides, item size, item type information, etc… They can be indexed by C integers, thus allowing fast access to the NumPy array data.\n",
    "\n",
    "```python\n",
    "cdef int [:] foo         # 1D memoryview\n",
    "cdef int [:, :] foo      # 2D memoryview\n",
    "cdef int [:, :, :] foo   # 3D memoryview\n",
    "...                      # You get the idea.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "def add_cy2(double[:, :] array_1, double[:, :] array_2):\n",
    "    # Py_ssize_t is the proper C type for Python array indices.\n",
    "    cdef Py_ssize_t x_max = array_1.shape[0]\n",
    "    cdef Py_ssize_t y_max = array_1.shape[1]\n",
    "    result = np.zeros((x_max, y_max))\n",
    "    cdef double[:, :] result_view = result\n",
    "    cdef Py_ssize_t x, y\n",
    "    for x in range(x_max):\n",
    "        for y in range(y_max):\n",
    "            result_view[x, y] = array_1[x, y]+array_2[x, y]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_cy2(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "cimport cython\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "def add_cy3(double[:, ::1] array_1, double[:, ::1] array_2): #C contiguous memoryview\n",
    "    cdef Py_ssize_t x_max = array_1.shape[0]\n",
    "    cdef Py_ssize_t y_max = array_1.shape[1]\n",
    "    result = np.zeros((x_max, y_max))\n",
    "    cdef double[:, ::1] result_view = result\n",
    "    cdef Py_ssize_t x, y\n",
    "    for x in range(x_max):\n",
    "        for y in range(y_max):\n",
    "            result_view[x, y] = array_1[x, y]+array_2[x, y]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_cy3(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still slower than Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Parallelism: ```prange```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a --compile-args=-fopenmp --link-args=-fopenmp\n",
    "from cython.parallel import prange\n",
    "import numpy as np\n",
    "cimport cython\n",
    "@cython.boundscheck(False)  # Deactivate bounds checking\n",
    "@cython.wraparound(False)   # Deactivate negative indexing.\n",
    "def add_cy4(double[:, ::1] array_1, double[:, ::1] array_2):\n",
    "    cdef Py_ssize_t x_max = array_1.shape[0]\n",
    "    cdef Py_ssize_t y_max = array_1.shape[1]\n",
    "    result = np.zeros((x_max, y_max))\n",
    "    cdef double[:, ::1] result_view = result\n",
    "    cdef Py_ssize_t x, y\n",
    "    for x in prange(x_max,nogil=True):\n",
    "        for y in range(y_max):\n",
    "            result_view[x, y] = array_1[x, y]+array_2[x, y]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_cy4(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_numpy(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.random.randn(10000,10000)\n",
    "b=np.random.randn(10000,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should we use Cython?\n",
    "\n",
    "- definitely not for ```a+b```; most useful for speeding up operations that can't be easily vectorized\n",
    "- Use Cython for bottlenecks, rather than re-writing everything in Cython\n",
    "- Numba ```@njit``` is an alternative, but sometimes not feasible, an example: https://docs.scipy.org/doc/scipy/reference/optimize.cython_optimize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dask-horizontal.svg\" \n",
    "     align=\"left\"\n",
    "     width=\"20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import dask\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A flexible library for parallel computing in Python\n",
    "\n",
    "- Dynamic task scheduling optimized for computation\n",
    "- “Big Data” collections like dask arrays (extending NumPy interfaces to larger-than-memory or distributed environments). These parallel collections run on top of dynamic task schedulers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dask-array-black-text.svg\" \n",
    "     align=\"left\"\n",
    "     width=\"40%\"\n",
    "     alt=\"Dask arrays are blocked numpy arrays\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid.\n",
    "- Dask arrays support most of the NumPy interface\n",
    "  - Arithmetic and scalar mathematics: ```+, *, exp, log, ...```\n",
    "  - Reductions along axes: ```sum(), mean(), std(), sum(axis=0), ...```\n",
    "  - Tensor contractions / dot products / matrix multiply: ```tensordot```\n",
    "  - Axis reordering / transpose: ```transpose```\n",
    "  - Slicing: ```x[:100, 500:100:-2]```\n",
    "  - Fancy indexing along single axes with lists or NumPy arrays: ```x[:, [10, 1, 5]]```\n",
    "  - Array protocols like ```__array__``` and ```__array_ufunc__```\n",
    "  - Some linear algebra: ```svd, qr, solve, solve_triangular, lstsq```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt=xr.open_dataset('/scratch/brown/kong97/WBGT/cmip6/ACCESS_CM2/WBGT_2000.nc')\n",
    "wbgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt.WBGT.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt=wbgt.chunk({'time':200})\n",
    "wbgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt_mean=wbgt.WBGT.mean('time')\n",
    "wbgt_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wbgt_mean=wbgt_mean.load()\n",
    "wbgt_mean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask has two families of task schedulers:\n",
    "* **Single machine scheduler:** Default scheduler, can only be used on a single machine. If you import Dask, set up a computation, and then call compute, then you will use the single-machine scheduler by default. ***We use single-machine scheduler above by default!***\n",
    "\n",
    "\n",
    "* **Distributed scheduler:** can run on a single machine or distributed across a cluster, **should be preferred even on a single machine** (offer more diagnostic features). To use the dask.distributed scheduler you must set up a Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(processes=False)\n",
    "#client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Threads**: One process, multiple threads; good for numeric code that releases the GIL (like NumPy, Pandas, Scikit-Learn, Numba, …)\n",
    "\n",
    "- **Processes**: several processes (maybe also multiple threads in one process); good for pure Python objects like strings or JSON-like dictionary data that holds onto the GIL; expensive inter-process communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use ```xr.open_mfdataset``` to open multiple files parallelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbgt2=xr.open_mfdataset('/scratch/brown/kong97/WBGT/cmip6/ACCESS_CM2/WBGT*.nc',parallel=True)\n",
    "wbgt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply customized function to dask arrays chunk by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def wbgtlim(M):\n",
    "    return 56.7-11.5*np.log10(M)\n",
    "\n",
    "@njit\n",
    "def workability(WBGT,M):\n",
    "    WBGTlimrest=wbgtlim(117)\n",
    "    WBGTlim=wbgtlim(M)\n",
    "    return np.maximum(0,np.minimum(1,(WBGTlimrest-WBGT)/(WBGTlimrest-WBGTlim)))\n",
    "def labor(WBGT,M):\n",
    "    lc=xr.apply_ufunc(workability,WBGT-273.15,M,dask=\"parallelized\",output_dtypes=[float])\n",
    "    lc=lc.rename({'WBGT':'labor_capacity'})\n",
    "    return lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc=labor(wbgt,400)\n",
    "lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lc=lc.load()\n",
    "#or write to disk directly\n",
    "#lc.to_netcdf('/scratch/brown/kong97/labor.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tips\n",
    "\n",
    "- select good chunksize\n",
    "  - small enough so that many chunks can fit in memory at once\n",
    "  - large enough to avoid overhead (rare to see chunk size below 100MB)\n",
    "- orient your chunk\n",
    "  - the way we chunk matters; if we often slice along 'time' dimension, it's better to chunk along it.\n",
    "- avoid too many tasks\n",
    "  - every task comes with overhead (200us ~ 1ms);  millions of tasks lead to overhead of 10 minutes ~ hours\n",
    "  - easy to create too many tasks: ```array_a+1``` can create many new tasks\n",
    "  - avoid too small chunks\n",
    "  - Fusing operations together and use ```xr.apply_ufunc()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "cluster = SLURMCluster(\n",
    "    queue=\"huberm\",\n",
    "    cores=24,\n",
    "    processes=1,\n",
    "    local_directory='/tmp',\n",
    "    project=\"huberm\",\n",
    "    memory=\"80 GB\",\n",
    "    walltime=\"00:30:00\",\n",
    "    interface='ib0' # choose the faster network\n",
    ")\n",
    "\n",
    "client=Client(cluster)\n",
    "cluster.scale(5)\n",
    "cluster.adapt(minimum=2, maximum=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine Cython and Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import intake\n",
    "import zarr\n",
    "import pandas as pd\n",
    "from wetbulb import wetbulb\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "df_data=df.query(\"source_id=='GFDL-CM4'& experiment_id=='historical' & table_id=='3hr' & variable_id==['tas', 'huss','ps'] & grid_label=='gr1'\")\n",
    "def drop_all_bounds(ds):\n",
    "    \"\"\"Drop coordinates like 'time_bounds' from datasets,\n",
    "    which can lead to issues when merging.\"\"\"\n",
    "    drop_vars = [vname for vname in ds.coords\n",
    "                 if (('_bounds') in vname ) or ('_bnds') in vname or ('height') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "def open_dsets(df):\n",
    "    \"\"\"Open datasets from cloud storage and return xarray dataset.\"\"\"\n",
    "    dsets = [xr.open_zarr(fsspec.get_mapper(ds_url), consolidated=True)\n",
    "             .pipe(drop_all_bounds)\n",
    "             for ds_url in df.zstore]\n",
    "    return dsets\n",
    "dsets = dict() \n",
    "for group, df in df_data.groupby(by=['variable_id']):\n",
    "    dsets[group] = open_dsets(df)\n",
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets['tas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huss=dsets['huss'][0].huss.sel(time='2001') #specific humidity\n",
    "tas=dsets['tas'][0].tas.sel(time='2001') # 2m air temperature\n",
    "ps=dsets['ps'][0].ps.sel(time='2001') # surface air pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb=xr.apply_ufunc(wetbulb,tas.astype('float64'),huss.astype('float64'), ps.astype('float64'),dask=\"parallelized\",output_dtypes=[float])\n",
    "wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wb=wb.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qin",
   "language": "python",
   "name": "qin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
